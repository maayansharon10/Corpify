{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.metrics as skmetrics\n",
    "import krippendorff as kd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# train set\n",
    "# bi grams - on target and on src -> 2 and 3 grams\n",
    "\n",
    "https://practicaldatascience.co.uk/machine-learning/how-to-use-count-vectorization-for-n-gram-analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "outputs": [],
   "source": [
    "def get_ngrams(text, ngram_from=2, ngram_to=2, n=None, max_features=20000):\n",
    "    vec = CountVectorizer(ngram_range=(ngram_from, ngram_to),\n",
    "                          max_features=max_features,\n",
    "                          stop_words='english'\n",
    ").fit(text)\n",
    "    print(vec)\n",
    "    bag_of_words = vec.transform(text)\n",
    "    sum_words = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, sum_words[0, i]) for word, i in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return words_freq[:n]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "outputs": [
    {
     "data": {
      "text/plain": "      id                                             source   \n0    335              Stop acting like you know everything.  \\\n1    239  I'm not interested in hanging out with you in ...   \n2    104     I couldn't be happier to decline this invite.    \n3    111         I don't even understand what your'e asking   \n4    341      Stop being a pushover. Stand up for yourself.   \n..   ...                                                ...   \n503   71                                   He works for me.   \n504  106                                 I deserve a raise.   \n505  273               Leave me alone and let me do my job.   \n506  439                                    This is urgent.   \n507  102                 I can't trust them with this task.   \n\n                                                target  \n0    Your knowledge is appreciated, but let's ensur...  \n1    I appreciate the invite, but I am completely b...  \n2                         Darn! not able to fit it in.  \n3         Could you just clarify your question for me?  \n4    Advocating for oneself is an important skill i...  \n..                                                 ...  \n503                                  We work together.  \n504  Given my contributions to the company's succes...  \n505  Some space to complete my work that would be v...  \n506        Please prioritize this as soon as possible.  \n507  Make sure to keep an eye on their progress on ...  \n\n[508 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>source</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>335</td>\n      <td>Stop acting like you know everything.</td>\n      <td>Your knowledge is appreciated, but let's ensur...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>239</td>\n      <td>I'm not interested in hanging out with you in ...</td>\n      <td>I appreciate the invite, but I am completely b...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>104</td>\n      <td>I couldn't be happier to decline this invite.</td>\n      <td>Darn! not able to fit it in.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>111</td>\n      <td>I don't even understand what your'e asking</td>\n      <td>Could you just clarify your question for me?</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>341</td>\n      <td>Stop being a pushover. Stand up for yourself.</td>\n      <td>Advocating for oneself is an important skill i...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>71</td>\n      <td>He works for me.</td>\n      <td>We work together.</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>106</td>\n      <td>I deserve a raise.</td>\n      <td>Given my contributions to the company's succes...</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>273</td>\n      <td>Leave me alone and let me do my job.</td>\n      <td>Some space to complete my work that would be v...</td>\n    </tr>\n    <tr>\n      <th>506</th>\n      <td>439</td>\n      <td>This is urgent.</td>\n      <td>Please prioritize this as soon as possible.</td>\n    </tr>\n    <tr>\n      <th>507</th>\n      <td>102</td>\n      <td>I can't trust them with this task.</td>\n      <td>Make sure to keep an eye on their progress on ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>508 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(r\"Evaluation_Data_And_Annotation\\train_data.csv\")  # need to change train_data to location\n",
    "df_train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=20000, ngram_range=(2, 2), stop_words='english')\n"
     ]
    },
    {
     "data": {
      "text/plain": "                Bigram  Frequency\n0             don want          8\n1          sounds like          7\n2          stop trying          5\n3         wasting time          5\n4           waste time          5\n5       don understand          4\n6              time ll          4\n7             don know          4\n8             don need          4\n9          stop acting          3\n10         acting like          3\n11           free time          3\n12         need answer          3\n13  stop micromanaging          3\n14          read email          3\n15            don time          3\n16       stop ignoring          3\n17             don try          3\n18          make sense          3\n19           doing job          3",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bigram</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>don want</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sounds like</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>stop trying</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>wasting time</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>waste time</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>don understand</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>time ll</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>don know</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>don need</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>stop acting</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>acting like</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>free time</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>need answer</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>stop micromanaging</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>read email</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>don time</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>stop ignoring</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>don try</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>make sense</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>doing job</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top 20 2-grams for train set - source\n",
    "bigrams_source = get_ngrams(df_train['source'], ngram_from=2, ngram_to=2, n=20)\n",
    "bigrams_df_train_source = pd.DataFrame(bigrams_source)\n",
    "bigrams_df_train_source.columns = [\"Bigram\", \"Frequency\"]\n",
    "bigrams_df_train_source.to_csv(r\"Evaluation_Data_And_Annotation/ngrams_results/train_set_source_2bigrams.csv\")\n",
    "bigrams_df_train_source"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=20000, ngram_range=(3, 3), stop_words='english')\n"
     ]
    },
    {
     "data": {
      "text/plain": "                     Bigram  Frequency\n0          stop acting like          3\n1    answer questions asked          2\n2      questions asked just          2\n3           really don want          2\n4          doesn make sense          2\n5          acting like know          1\n6   interested hanging free          1\n7         hanging free time          1\n8    couldn happier decline          1\n9    happier decline invite          1\n10    don understand asking          1\n11      stop pushover stand          1\n12       gonna present work          1\n13      present work credit          1\n14          want accept job          1\n15           accept job pay          1\n16            job pay asked          1\n17  sending email thousands          1\n18   email thousands people          1\n19    thousands people need          1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bigram</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>stop acting like</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>answer questions asked</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>questions asked just</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>really don want</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>doesn make sense</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>acting like know</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>interested hanging free</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>hanging free time</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>couldn happier decline</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>happier decline invite</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>don understand asking</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>stop pushover stand</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>gonna present work</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>present work credit</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>want accept job</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>accept job pay</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>job pay asked</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>sending email thousands</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>email thousands people</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>thousands people need</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top 20 3-grams for train set - source\n",
    "three_grams_source = get_ngrams(df_train['source'], ngram_from=3, ngram_to=3, n=20)\n",
    "three_grams_df_train_source = pd.DataFrame(three_grams_source)\n",
    "three_grams_df_train_source.columns = [\"Bigram\", \"Frequency\"]\n",
    "three_grams_df_train_source.to_csv(r\"Evaluation_Data_And_Annotation/ngrams_results/train_set_source_3bigrams.csv\")\n",
    "three_grams_df_train_source"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=20000, ngram_range=(2, 2), stop_words='english')\n"
     ]
    },
    {
     "data": {
      "text/plain": "                      Bigram  Frequency\n0                   let know         14\n1                 let ensure          9\n2               work related          8\n3              shared spaces          8\n4           keeping personal          6\n5             personal items          6\n6                  make sure          6\n7              working hours          6\n8             helps maintain          5\n9   maintaining professional          5\n10            moving forward          5\n11                 feel free          5\n12                 don think          5\n13                 good time          4\n14      information provided          4\n15             help maintain          4\n16     maintain professional          4\n17              helps ensure          4\n18           help understand          4\n19               let discuss          4",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bigram</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>let know</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>let ensure</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>work related</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>shared spaces</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>keeping personal</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>personal items</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>make sure</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>working hours</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>helps maintain</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>maintaining professional</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>moving forward</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>feel free</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>don think</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>good time</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>information provided</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>help maintain</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>maintain professional</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>helps ensure</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>help understand</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>let discuss</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top 20 2-grams for train set - target\n",
    "bigrams_target = get_ngrams(df_train['target'], ngram_from=2, ngram_to=2, n=20)\n",
    "bigrams_df_test_target = pd.DataFrame(bigrams_target)\n",
    "bigrams_df_test_target.columns = [\"Bigram\", \"Frequency\"]\n",
    "bigrams_df_test_target.to_csv(r\"Evaluation_Data_And_Annotation/ngrams_results/train_set_target_2bigrams.csv\")\n",
    "bigrams_df_test_target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(max_features=20000, ngram_range=(3, 3), stop_words='english')\n"
     ]
    },
    {
     "data": {
      "text/plain": "                             Bigram  Frequency\n0             focusing work related          3\n1              sorry committed hope          2\n2         committed hope understand          2\n3       order successfully complete          2\n4      sharing personal experiences          2\n5           maintaining focus tasks          2\n6                respect time helps          2\n7             pleasant shared space          2\n8        help maintain professional          2\n9       maintain professional focus          2\n10              respectful time let          2\n11           thank sharing thoughts          2\n12  disconnect information provided          2\n13             workload quite heavy          2\n14                    input ll mind          2\n15                  ll mind forward          2\n16                  let know better          2\n17                  know better way          2\n18               better way contact          2\n19           resolved soon possible          2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Bigram</th>\n      <th>Frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>focusing work related</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sorry committed hope</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>committed hope understand</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>order successfully complete</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>sharing personal experiences</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>maintaining focus tasks</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>respect time helps</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>pleasant shared space</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>help maintain professional</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>maintain professional focus</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>respectful time let</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>thank sharing thoughts</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>disconnect information provided</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>workload quite heavy</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>input ll mind</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>ll mind forward</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>let know better</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>know better way</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>better way contact</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>resolved soon possible</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get top 20 3-grams for train set - target\n",
    "three_grams_target = get_ngrams(df_train['target'], ngram_from=3, ngram_to=3, n=20)\n",
    "three_grams_df_test_target = pd.DataFrame(three_grams_target)\n",
    "three_grams_df_test_target.columns = [\"Bigram\", \"Frequency\"]\n",
    "three_grams_df_test_target.to_csv(r\"Evaluation_Data_And_Annotation/ngrams_results/train_set_target_3bigrams.csv\")\n",
    "three_grams_df_test_target"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# human annotation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "outputs": [],
   "source": [
    "df_r1 = pd.read_csv(r\"Corpify_comparing_sents_Maayan.csv\")  # rater no.1\n",
    "df_r2 = pd.read_csv(r\"Corpify_comparing_sents_Nitzan.csv\")  # rater no.2\n",
    "remove_cols = [\"Corpy_detection_target\", \"Corpy_detection_t5detox\", \"Corpy_detection_t5formal\",\n",
    "               \"Corpy_detection_t5large\", \"Corpy_detection_flanlarge\"]\n",
    "df_r1 = df_r1.drop(columns=remove_cols)\n",
    "df_r2 = df_r2.drop(columns=remove_cols)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      src   \ncount                                                  64  \\\nunique                                                 64   \ntop     I don't get paid enough to deal with this buls...   \nfreq                                                    1   \nmean                                                  NaN   \nstd                                                   NaN   \nmin                                                   NaN   \n25%                                                   NaN   \n50%                                                   NaN   \n75%                                                   NaN   \nmax                                                   NaN   \n\n                                                   target   \ncount                                                  64  \\\nunique                                                 64   \ntop     This falls outside my responsibilities but I w...   \nfreq                                                    1   \nmean                                                  NaN   \nstd                                                   NaN   \nmin                                                   NaN   \n25%                                                   NaN   \n50%                                                   NaN   \n75%                                                   NaN   \nmax                                                   NaN   \n\n                                           pred_t5detox   \ncount                                                63  \\\nunique                                               63   \ntop     I'm not able to get paid enough to handle this.   \nfreq                                                  1   \nmean                                                NaN   \nstd                                                 NaN   \nmin                                                 NaN   \n25%                                                 NaN   \n50%                                                 NaN   \n75%                                                 NaN   \nmax                                                 NaN   \n\n                                   pred_t5formal   \ncount                                         64  \\\nunique                                        61   \ntop     I'm not sure I understand your question.   \nfreq                                           2   \nmean                                         NaN   \nstd                                          NaN   \nmin                                          NaN   \n25%                                          NaN   \n50%                                          NaN   \n75%                                          NaN   \nmax                                          NaN   \n\n                                             pred_t5large   \ncount                                                  64  \\\nunique                                                 64   \ntop     I'm not able to take this on as I'm not able t...   \nfreq                                                    1   \nmean                                                  NaN   \nstd                                                   NaN   \nmin                                                   NaN   \n25%                                                   NaN   \n50%                                                   NaN   \n75%                                                   NaN   \nmax                                                   NaN   \n\n                                     pred_flanlarge  Fluency_target   \ncount                                            64            64.0  \\\nunique                                           52             NaN   \ntop     I'm not sure I'm the right person for that.             NaN   \nfreq                                              9             NaN   \nmean                                            NaN             2.0   \nstd                                             NaN             0.0   \nmin                                             NaN             2.0   \n25%                                             NaN             2.0   \n50%                                             NaN             2.0   \n75%                                             NaN             2.0   \nmax                                             NaN             2.0   \n\n        Fluency_t5detox  Fluency_t5formal  Fluency_t5large  ...   \ncount         64.000000         64.000000         64.00000  ...  \\\nunique              NaN               NaN              NaN  ...   \ntop                 NaN               NaN              NaN  ...   \nfreq                NaN               NaN              NaN  ...   \nmean           1.812500          1.859375          1.78125  ...   \nstd            0.530798          0.499752          0.57649  ...   \nmin            0.000000          0.000000          0.00000  ...   \n25%            2.000000          2.000000          2.00000  ...   \n50%            2.000000          2.000000          2.00000  ...   \n75%            2.000000          2.000000          2.00000  ...   \nmax            2.000000          2.000000          2.00000  ...   \n\n        content_intention_target  content_intention_t5detox   \ncount                  64.000000                  64.000000  \\\nunique                       NaN                        NaN   \ntop                          NaN                        NaN   \nfreq                         NaN                        NaN   \nmean                    1.968750                   1.125000   \nstd                     0.175368                   0.934353   \nmin                     1.000000                   0.000000   \n25%                     2.000000                   0.000000   \n50%                     2.000000                   1.500000   \n75%                     2.000000                   2.000000   \nmax                     2.000000                   2.000000   \n\n        content_intention_t5formal  content_intention_t5large   \ncount                    64.000000                  64.000000  \\\nunique                         NaN                        NaN   \ntop                            NaN                        NaN   \nfreq                           NaN                        NaN   \nmean                      1.171875                   1.218750   \nstd                       0.882901                   0.933822   \nmin                       0.000000                   0.000000   \n25%                       0.000000                   0.000000   \n50%                       1.000000                   2.000000   \n75%                       2.000000                   2.000000   \nmax                       2.000000                   2.000000   \n\n        content_intention_flanlarge  essential_details_target   \ncount                     64.000000                 64.000000  \\\nunique                          NaN                       NaN   \ntop                             NaN                       NaN   \nfreq                            NaN                       NaN   \nmean                       0.890625                  1.921875   \nstd                        0.961434                  0.323899   \nmin                        0.000000                  0.000000   \n25%                        0.000000                  2.000000   \n50%                        0.000000                  2.000000   \n75%                        2.000000                  2.000000   \nmax                        2.000000                  2.000000   \n\n        essential_details_t5detox  essential_details_t5formal   \ncount                   64.000000                   64.000000  \\\nunique                        NaN                         NaN   \ntop                           NaN                         NaN   \nfreq                          NaN                         NaN   \nmean                     1.046875                    1.250000   \nstd                      0.915990                    0.872872   \nmin                      0.000000                    0.000000   \n25%                      0.000000                    0.000000   \n50%                      1.000000                    2.000000   \n75%                      2.000000                    2.000000   \nmax                      2.000000                    2.000000   \n\n        essential_details_t5large  essential_details_flanlarge  \ncount                   64.000000                    64.000000  \nunique                        NaN                          NaN  \ntop                           NaN                          NaN  \nfreq                          NaN                          NaN  \nmean                     1.187500                     0.937500  \nstd                      0.940702                     0.940702  \nmin                      0.000000                     0.000000  \n25%                      0.000000                     0.000000  \n50%                      2.000000                     1.000000  \n75%                      2.000000                     2.000000  \nmax                      2.000000                     2.000000  \n\n[11 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>src</th>\n      <th>target</th>\n      <th>pred_t5detox</th>\n      <th>pred_t5formal</th>\n      <th>pred_t5large</th>\n      <th>pred_flanlarge</th>\n      <th>Fluency_target</th>\n      <th>Fluency_t5detox</th>\n      <th>Fluency_t5formal</th>\n      <th>Fluency_t5large</th>\n      <th>...</th>\n      <th>content_intention_target</th>\n      <th>content_intention_t5detox</th>\n      <th>content_intention_t5formal</th>\n      <th>content_intention_t5large</th>\n      <th>content_intention_flanlarge</th>\n      <th>essential_details_target</th>\n      <th>essential_details_t5detox</th>\n      <th>essential_details_t5formal</th>\n      <th>essential_details_t5large</th>\n      <th>essential_details_flanlarge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>64</td>\n      <td>64</td>\n      <td>63</td>\n      <td>64</td>\n      <td>64</td>\n      <td>64</td>\n      <td>64.0</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.00000</td>\n      <td>...</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>64</td>\n      <td>64</td>\n      <td>63</td>\n      <td>61</td>\n      <td>64</td>\n      <td>52</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>I don't get paid enough to deal with this buls...</td>\n      <td>This falls outside my responsibilities but I w...</td>\n      <td>I'm not able to get paid enough to handle this.</td>\n      <td>I'm not sure I understand your question.</td>\n      <td>I'm not able to take this on as I'm not able t...</td>\n      <td>I'm not sure I'm the right person for that.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.812500</td>\n      <td>1.859375</td>\n      <td>1.78125</td>\n      <td>...</td>\n      <td>1.968750</td>\n      <td>1.125000</td>\n      <td>1.171875</td>\n      <td>1.218750</td>\n      <td>0.890625</td>\n      <td>1.921875</td>\n      <td>1.046875</td>\n      <td>1.250000</td>\n      <td>1.187500</td>\n      <td>0.937500</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.530798</td>\n      <td>0.499752</td>\n      <td>0.57649</td>\n      <td>...</td>\n      <td>0.175368</td>\n      <td>0.934353</td>\n      <td>0.882901</td>\n      <td>0.933822</td>\n      <td>0.961434</td>\n      <td>0.323899</td>\n      <td>0.915990</td>\n      <td>0.872872</td>\n      <td>0.940702</td>\n      <td>0.940702</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>1.500000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>11 rows × 26 columns</p>\n</div>"
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r1.describe(include=\"all\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                      src   \ncount                                                  64  \\\nunique                                                 64   \ntop     I don't get paid enough to deal with this buls...   \nfreq                                                    1   \nmean                                                  NaN   \nstd                                                   NaN   \nmin                                                   NaN   \n25%                                                   NaN   \n50%                                                   NaN   \n75%                                                   NaN   \nmax                                                   NaN   \n\n                                                   target   \ncount                                                  64  \\\nunique                                                 64   \ntop     This falls outside my responsibilities but I w...   \nfreq                                                    1   \nmean                                                  NaN   \nstd                                                   NaN   \nmin                                                   NaN   \n25%                                                   NaN   \n50%                                                   NaN   \n75%                                                   NaN   \nmax                                                   NaN   \n\n                                           pred_t5detox   \ncount                                                63  \\\nunique                                               63   \ntop     I'm not able to get paid enough to handle this.   \nfreq                                                  1   \nmean                                                NaN   \nstd                                                 NaN   \nmin                                                 NaN   \n25%                                                 NaN   \n50%                                                 NaN   \n75%                                                 NaN   \nmax                                                 NaN   \n\n                                   pred_t5formal   \ncount                                         64  \\\nunique                                        61   \ntop     I'm not sure I understand your question.   \nfreq                                           2   \nmean                                         NaN   \nstd                                          NaN   \nmin                                          NaN   \n25%                                          NaN   \n50%                                          NaN   \n75%                                          NaN   \nmax                                          NaN   \n\n                                             pred_t5large   \ncount                                                  64  \\\nunique                                                 64   \ntop     I'm not able to take this on as I'm not able t...   \nfreq                                                    1   \nmean                                                  NaN   \nstd                                                   NaN   \nmin                                                   NaN   \n25%                                                   NaN   \n50%                                                   NaN   \n75%                                                   NaN   \nmax                                                   NaN   \n\n                                     pred_flanlarge  Fluency_target   \ncount                                            64            64.0  \\\nunique                                           52             NaN   \ntop     I'm not sure I'm the right person for that.             NaN   \nfreq                                              9             NaN   \nmean                                            NaN             2.0   \nstd                                             NaN             0.0   \nmin                                             NaN             2.0   \n25%                                             NaN             2.0   \n50%                                             NaN             2.0   \n75%                                             NaN             2.0   \nmax                                             NaN             2.0   \n\n        Fluency_t5detox  Fluency_t5formal  Fluency_t5large  ...   \ncount          64.00000         64.000000        64.000000  ...  \\\nunique              NaN               NaN              NaN  ...   \ntop                 NaN               NaN              NaN  ...   \nfreq                NaN               NaN              NaN  ...   \nmean            1.78125          1.875000         1.750000  ...   \nstd             0.57649          0.454257         0.563436  ...   \nmin             0.00000          0.000000         0.000000  ...   \n25%             2.00000          2.000000         2.000000  ...   \n50%             2.00000          2.000000         2.000000  ...   \n75%             2.00000          2.000000         2.000000  ...   \nmax             2.00000          2.000000         2.000000  ...   \n\n        content_intention_target  content_intention_t5detox   \ncount                  64.000000                  64.000000  \\\nunique                       NaN                        NaN   \ntop                          NaN                        NaN   \nfreq                         NaN                        NaN   \nmean                    1.984375                   1.125000   \nstd                     0.125000                   0.863731   \nmin                     1.000000                   0.000000   \n25%                     2.000000                   0.000000   \n50%                     2.000000                   1.000000   \n75%                     2.000000                   2.000000   \nmax                     2.000000                   2.000000   \n\n        content_intention_t5formal  content_intention_t5large   \ncount                     64.00000                  64.000000  \\\nunique                         NaN                        NaN   \ntop                            NaN                        NaN   \nfreq                           NaN                        NaN   \nmean                       1.37500                   1.390625   \nstd                        0.82616                   0.847352   \nmin                        0.00000                   0.000000   \n25%                        1.00000                   1.000000   \n50%                        2.00000                   2.000000   \n75%                        2.00000                   2.000000   \nmax                        2.00000                   2.000000   \n\n        content_intention_flanlarge  essential_details_target   \ncount                     64.000000                 64.000000  \\\nunique                          NaN                       NaN   \ntop                             NaN                       NaN   \nfreq                            NaN                       NaN   \nmean                       1.031250                  1.718750   \nstd                        0.925284                  0.548266   \nmin                        0.000000                  0.000000   \n25%                        0.000000                  2.000000   \n50%                        1.000000                  2.000000   \n75%                        2.000000                  2.000000   \nmax                        2.000000                  2.000000   \n\n        essential_details_t5detox  essential_details_t5formal   \ncount                   64.000000                   64.000000  \\\nunique                        NaN                         NaN   \ntop                           NaN                         NaN   \nfreq                          NaN                         NaN   \nmean                     1.031250                    1.140625   \nstd                      0.815889                    0.833185   \nmin                      0.000000                    0.000000   \n25%                      0.000000                    0.000000   \n50%                      1.000000                    1.000000   \n75%                      2.000000                    2.000000   \nmax                      2.000000                    2.000000   \n\n        essential_details_t5large  essential_details_flanlarge  \ncount                     64.0000                    64.000000  \nunique                        NaN                          NaN  \ntop                           NaN                          NaN  \nfreq                          NaN                          NaN  \nmean                       1.4375                     0.875000  \nstd                        0.7533                     0.863731  \nmin                        0.0000                     0.000000  \n25%                        1.0000                     0.000000  \n50%                        2.0000                     1.000000  \n75%                        2.0000                     2.000000  \nmax                        2.0000                     2.000000  \n\n[11 rows x 26 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>src</th>\n      <th>target</th>\n      <th>pred_t5detox</th>\n      <th>pred_t5formal</th>\n      <th>pred_t5large</th>\n      <th>pred_flanlarge</th>\n      <th>Fluency_target</th>\n      <th>Fluency_t5detox</th>\n      <th>Fluency_t5formal</th>\n      <th>Fluency_t5large</th>\n      <th>...</th>\n      <th>content_intention_target</th>\n      <th>content_intention_t5detox</th>\n      <th>content_intention_t5formal</th>\n      <th>content_intention_t5large</th>\n      <th>content_intention_flanlarge</th>\n      <th>essential_details_target</th>\n      <th>essential_details_t5detox</th>\n      <th>essential_details_t5formal</th>\n      <th>essential_details_t5large</th>\n      <th>essential_details_flanlarge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>64</td>\n      <td>64</td>\n      <td>63</td>\n      <td>64</td>\n      <td>64</td>\n      <td>64</td>\n      <td>64.0</td>\n      <td>64.00000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>...</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.00000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.000000</td>\n      <td>64.0000</td>\n      <td>64.000000</td>\n    </tr>\n    <tr>\n      <th>unique</th>\n      <td>64</td>\n      <td>64</td>\n      <td>63</td>\n      <td>61</td>\n      <td>64</td>\n      <td>52</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>top</th>\n      <td>I don't get paid enough to deal with this buls...</td>\n      <td>This falls outside my responsibilities but I w...</td>\n      <td>I'm not able to get paid enough to handle this.</td>\n      <td>I'm not sure I understand your question.</td>\n      <td>I'm not able to take this on as I'm not able t...</td>\n      <td>I'm not sure I'm the right person for that.</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>freq</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>1.78125</td>\n      <td>1.875000</td>\n      <td>1.750000</td>\n      <td>...</td>\n      <td>1.984375</td>\n      <td>1.125000</td>\n      <td>1.37500</td>\n      <td>1.390625</td>\n      <td>1.031250</td>\n      <td>1.718750</td>\n      <td>1.031250</td>\n      <td>1.140625</td>\n      <td>1.4375</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.0</td>\n      <td>0.57649</td>\n      <td>0.454257</td>\n      <td>0.563436</td>\n      <td>...</td>\n      <td>0.125000</td>\n      <td>0.863731</td>\n      <td>0.82616</td>\n      <td>0.847352</td>\n      <td>0.925284</td>\n      <td>0.548266</td>\n      <td>0.815889</td>\n      <td>0.833185</td>\n      <td>0.7533</td>\n      <td>0.863731</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>1.00000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>2.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>2.0000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.0000</td>\n      <td>2.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>...</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.00000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.000000</td>\n      <td>2.0000</td>\n      <td>2.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>11 rows × 26 columns</p>\n</div>"
     },
     "execution_count": 409,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_r2.describe(include='all')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# human evaluation -\n",
    "### agreement scores"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# make sure both annotators have exact same columns and same line lengths\n",
    "print(df_r1.columns.tolist() == df_r2.columns.tolist())\n",
    "for col in df_r1.columns.tolist():\n",
    "    if len(df_r1[col]) != len(df_r2[col]):\n",
    "        print(f\"column {col} is not same length in both dfs\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fluency_target', 'Fluency_t5detox', 'Fluency_t5formal', 'Fluency_t5large', 'Fluency_flanlarge', 'Corpy_level_target', 'Corpy_level_t5detox', 'Corpy_level_t5formal', 'Corpy_level_t5large', 'Corpy_level_flanlarge', 'content_intention_target', 'content_intention_t5detox', 'content_intention_t5formal', 'content_intention_t5large', 'content_intention_flanlarge', 'essential_details_target', 'essential_details_t5detox', 'essential_details_t5formal', 'essential_details_t5large', 'essential_details_flanlarge']\n"
     ]
    }
   ],
   "source": [
    "# get only columns for measuring agreement\n",
    "data_no_eval_cols = ['src', 'target', \"pred_t5detox\", \"pred_t5formal\", \"pred_t5large\", \"pred_flanlarge\"]\n",
    "all_col_names = df_r1.columns.tolist()\n",
    "only_calc_col_names = [x for x in all_col_names if x not in data_no_eval_cols]\n",
    "print(only_calc_col_names)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "outputs": [],
   "source": [
    "def get_percentage_agreement_score(df_calc: pd.DataFrame, rater1_col: pd.DataFrame, rater2_col: pd.DataFrame) -> float:\n",
    "    df_calc['percentage_agreement_score'] = rater1_col == rater2_col\n",
    "    # number of agreements / total number of evaluations\n",
    "    return df_calc['percentage_agreement_score'].sum() / len(df_calc['percentage_agreement_score'])\n",
    "\n",
    "\n",
    "def get_cohen_kappar_score(rater1_col: pd.DataFrame, rater2_col: pd.DataFrame, ) -> float:\n",
    "    r1_list = rater1_col.to_list()\n",
    "    r2_list = rater2_col.to_list()\n",
    "    return 1 if r1_list == r2_list else skmetrics.cohen_kappa_score(r1_list, r2_list)\n",
    "\n",
    "\n",
    "def get_krippendorff_score(rater1_col: pd.DataFrame, rater2_col: pd.DataFrame, level_of_measurement) -> float:\n",
    "    if rater1_col.to_list() == rater2_col.to_list():\n",
    "        return 1\n",
    "    return kd.alpha([rater1_col, rater2_col], level_of_measurement=level_of_measurement)\n",
    "\n",
    "\n",
    "def get_dist_score(df_calc: pd.DataFrame, new_col: str, rater1_col: pd.DataFrame, rater2_col: pd.DataFrame) -> float:\n",
    "    df_calc[new_col] = abs(rater1_col - rater2_col)\n",
    "    return df_calc[new_col].mean()\n",
    "\n",
    "\n",
    "def calc_min_max_normalize(df: pd.DataFrame, col_name: str) -> pd.DataFrame:\n",
    "    # min - max normalize distance\n",
    "    return (df[col_name] - df[col_name].min()) / (\n",
    "            df[col_name].max() - df[col_name].min())\n",
    "    # return df[normalized_col_name]\n",
    "\n",
    "\n",
    "def get_raters_mean(rater1_col: pd.DataFrame, rater2_col: pd.DataFrame) -> float:\n",
    "    df_calc['mean_score'] = (rater1_col + rater2_col) / 2\n",
    "    return df_calc['mean_score'].mean()\n",
    "\n",
    "\n",
    "def get_raters_mean_std(rater1_col: pd.DataFrame, rater2_col: pd.DataFrame) -> float:\n",
    "    df_calc['mean_score'] = (rater1_col + rater2_col) / 2\n",
    "    return df_calc['mean_score'].std()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "outputs": [
    {
     "data": {
      "text/plain": "                         test  Fluency_target  Fluency_t5detox   \n0     percentage_of_agreement             1.0         0.921875  \\\n1           cohen_kappa_score             1.0         0.673469   \n2  krippendorff_score_ordinal             1.0         0.806287   \n3                        dist             0.0         0.093750   \n4             dist_normalized             NaN         0.046875   \n5         dist_normalized_var             NaN         0.029514   \n6         dist_normalized_std             NaN         0.171796   \n7                        mean             2.0         1.796875   \n8                    mean_std             0.0         0.524924   \n\n   Fluency_t5formal  Fluency_t5large  Fluency_flanlarge  Corpy_level_target   \n0          0.937500         0.890625           0.921875            0.937500  \\\n1          0.574043         0.621622           0.469320            0.308108   \n2          0.599294         0.736020           0.568891            0.300517   \n3          0.078125         0.125000           0.109375            0.093750   \n4          0.039062         0.062500           0.054688            0.052083   \n5          0.026228         0.035714           0.040613            0.039572   \n6          0.161950         0.188982           0.201526            0.198928   \n7          1.867188         1.765625           1.882812            1.937500   \n8          0.447588         0.534290           0.375248            0.243975   \n\n   Corpy_level_t5detox  Corpy_level_t5formal  Corpy_level_t5large  ...   \n0             0.546875              0.578125             0.515625  ...  \\\n1             0.346709              0.429138             0.371555  ...   \n2             0.676956              0.769589             0.678615  ...   \n3             0.703125              0.625000             0.734375  ...   \n4             0.175781              0.156250             0.183594  ...   \n5             0.056904              0.052579             0.060004  ...   \n6             0.238546              0.229302             0.244958  ...   \n7             0.757812              0.671875             0.414062  ...   \n8             1.394411              1.348776             1.424153  ...   \n\n   content_intention_target  content_intention_t5detox   \n0                  0.953125                   0.531250  \\\n1                 -0.021277                   0.259259   \n2                 -0.016000                   0.365425   \n3                  0.046875                   0.656250   \n4                  0.046875                   0.328125   \n5                  0.045387                   0.152530   \n6                  0.213042                   0.390551   \n7                  1.976562                   1.125000   \n8                  0.106521                   0.740013   \n\n   content_intention_t5formal  content_intention_t5large   \n0                    0.640625                   0.750000  \\\n1                    0.406930                   0.549296   \n2                    0.638775                   0.682651   \n3                    0.421875                   0.328125   \n4                    0.210938                   0.164062   \n5                    0.093688                   0.095672   \n6                    0.306085                   0.309309   \n7                    1.273438                   1.304688   \n8                    0.776284                   0.824319   \n\n   content_intention_flanlarge  essential_details_target   \n0                     0.765625                  0.781250  \\\n1                     0.609756                  0.197851   \n2                     0.763135                  0.196987   \n3                     0.296875                  0.265625   \n4                     0.148438                  0.132812   \n5                     0.084759                  0.073351   \n6                     0.291135                  0.270833   \n7                     0.960938                  1.820312   \n8                     0.887768                  0.349230   \n\n   essential_details_t5detox  essential_details_t5formal   \n0                   0.593750                    0.640625  \\\n1                   0.392257                    0.439452   \n2                   0.631924                    0.618098   \n3                   0.453125                    0.421875   \n4                   0.226562                    0.210938   \n5                   0.086744                    0.093688   \n6                   0.294523                    0.306085   \n7                   1.039062                    1.195312   \n8                   0.783281                    0.769545   \n\n   essential_details_t5large  essential_details_flanlarge  \n0                   0.671875                     0.718750  \n1                   0.449180                     0.558282  \n2                   0.625769                     0.780767  \n3                   0.406250                     0.312500  \n4                   0.203125                     0.156250  \n5                   0.100942                     0.070437  \n6                   0.317714                     0.265399  \n7                   1.312500                     0.906250  \n8                   0.774084                     0.849253  \n\n[9 rows x 21 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>test</th>\n      <th>Fluency_target</th>\n      <th>Fluency_t5detox</th>\n      <th>Fluency_t5formal</th>\n      <th>Fluency_t5large</th>\n      <th>Fluency_flanlarge</th>\n      <th>Corpy_level_target</th>\n      <th>Corpy_level_t5detox</th>\n      <th>Corpy_level_t5formal</th>\n      <th>Corpy_level_t5large</th>\n      <th>...</th>\n      <th>content_intention_target</th>\n      <th>content_intention_t5detox</th>\n      <th>content_intention_t5formal</th>\n      <th>content_intention_t5large</th>\n      <th>content_intention_flanlarge</th>\n      <th>essential_details_target</th>\n      <th>essential_details_t5detox</th>\n      <th>essential_details_t5formal</th>\n      <th>essential_details_t5large</th>\n      <th>essential_details_flanlarge</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>percentage_of_agreement</td>\n      <td>1.0</td>\n      <td>0.921875</td>\n      <td>0.937500</td>\n      <td>0.890625</td>\n      <td>0.921875</td>\n      <td>0.937500</td>\n      <td>0.546875</td>\n      <td>0.578125</td>\n      <td>0.515625</td>\n      <td>...</td>\n      <td>0.953125</td>\n      <td>0.531250</td>\n      <td>0.640625</td>\n      <td>0.750000</td>\n      <td>0.765625</td>\n      <td>0.781250</td>\n      <td>0.593750</td>\n      <td>0.640625</td>\n      <td>0.671875</td>\n      <td>0.718750</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cohen_kappa_score</td>\n      <td>1.0</td>\n      <td>0.673469</td>\n      <td>0.574043</td>\n      <td>0.621622</td>\n      <td>0.469320</td>\n      <td>0.308108</td>\n      <td>0.346709</td>\n      <td>0.429138</td>\n      <td>0.371555</td>\n      <td>...</td>\n      <td>-0.021277</td>\n      <td>0.259259</td>\n      <td>0.406930</td>\n      <td>0.549296</td>\n      <td>0.609756</td>\n      <td>0.197851</td>\n      <td>0.392257</td>\n      <td>0.439452</td>\n      <td>0.449180</td>\n      <td>0.558282</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>krippendorff_score_ordinal</td>\n      <td>1.0</td>\n      <td>0.806287</td>\n      <td>0.599294</td>\n      <td>0.736020</td>\n      <td>0.568891</td>\n      <td>0.300517</td>\n      <td>0.676956</td>\n      <td>0.769589</td>\n      <td>0.678615</td>\n      <td>...</td>\n      <td>-0.016000</td>\n      <td>0.365425</td>\n      <td>0.638775</td>\n      <td>0.682651</td>\n      <td>0.763135</td>\n      <td>0.196987</td>\n      <td>0.631924</td>\n      <td>0.618098</td>\n      <td>0.625769</td>\n      <td>0.780767</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>dist</td>\n      <td>0.0</td>\n      <td>0.093750</td>\n      <td>0.078125</td>\n      <td>0.125000</td>\n      <td>0.109375</td>\n      <td>0.093750</td>\n      <td>0.703125</td>\n      <td>0.625000</td>\n      <td>0.734375</td>\n      <td>...</td>\n      <td>0.046875</td>\n      <td>0.656250</td>\n      <td>0.421875</td>\n      <td>0.328125</td>\n      <td>0.296875</td>\n      <td>0.265625</td>\n      <td>0.453125</td>\n      <td>0.421875</td>\n      <td>0.406250</td>\n      <td>0.312500</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>dist_normalized</td>\n      <td>NaN</td>\n      <td>0.046875</td>\n      <td>0.039062</td>\n      <td>0.062500</td>\n      <td>0.054688</td>\n      <td>0.052083</td>\n      <td>0.175781</td>\n      <td>0.156250</td>\n      <td>0.183594</td>\n      <td>...</td>\n      <td>0.046875</td>\n      <td>0.328125</td>\n      <td>0.210938</td>\n      <td>0.164062</td>\n      <td>0.148438</td>\n      <td>0.132812</td>\n      <td>0.226562</td>\n      <td>0.210938</td>\n      <td>0.203125</td>\n      <td>0.156250</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>dist_normalized_var</td>\n      <td>NaN</td>\n      <td>0.029514</td>\n      <td>0.026228</td>\n      <td>0.035714</td>\n      <td>0.040613</td>\n      <td>0.039572</td>\n      <td>0.056904</td>\n      <td>0.052579</td>\n      <td>0.060004</td>\n      <td>...</td>\n      <td>0.045387</td>\n      <td>0.152530</td>\n      <td>0.093688</td>\n      <td>0.095672</td>\n      <td>0.084759</td>\n      <td>0.073351</td>\n      <td>0.086744</td>\n      <td>0.093688</td>\n      <td>0.100942</td>\n      <td>0.070437</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>dist_normalized_std</td>\n      <td>NaN</td>\n      <td>0.171796</td>\n      <td>0.161950</td>\n      <td>0.188982</td>\n      <td>0.201526</td>\n      <td>0.198928</td>\n      <td>0.238546</td>\n      <td>0.229302</td>\n      <td>0.244958</td>\n      <td>...</td>\n      <td>0.213042</td>\n      <td>0.390551</td>\n      <td>0.306085</td>\n      <td>0.309309</td>\n      <td>0.291135</td>\n      <td>0.270833</td>\n      <td>0.294523</td>\n      <td>0.306085</td>\n      <td>0.317714</td>\n      <td>0.265399</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>mean</td>\n      <td>2.0</td>\n      <td>1.796875</td>\n      <td>1.867188</td>\n      <td>1.765625</td>\n      <td>1.882812</td>\n      <td>1.937500</td>\n      <td>0.757812</td>\n      <td>0.671875</td>\n      <td>0.414062</td>\n      <td>...</td>\n      <td>1.976562</td>\n      <td>1.125000</td>\n      <td>1.273438</td>\n      <td>1.304688</td>\n      <td>0.960938</td>\n      <td>1.820312</td>\n      <td>1.039062</td>\n      <td>1.195312</td>\n      <td>1.312500</td>\n      <td>0.906250</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>mean_std</td>\n      <td>0.0</td>\n      <td>0.524924</td>\n      <td>0.447588</td>\n      <td>0.534290</td>\n      <td>0.375248</td>\n      <td>0.243975</td>\n      <td>1.394411</td>\n      <td>1.348776</td>\n      <td>1.424153</td>\n      <td>...</td>\n      <td>0.106521</td>\n      <td>0.740013</td>\n      <td>0.776284</td>\n      <td>0.824319</td>\n      <td>0.887768</td>\n      <td>0.349230</td>\n      <td>0.783281</td>\n      <td>0.769545</td>\n      <td>0.774084</td>\n      <td>0.849253</td>\n    </tr>\n  </tbody>\n</table>\n<p>9 rows × 21 columns</p>\n</div>"
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new df with results of annotator agreement\n",
    "res_agreement = {\"test\": [\"percentage_of_agreement\",\n",
    "                          \"cohen_kappa_score\",\n",
    "                          \"krippendorff_score_ordinal\",\n",
    "                          \"dist\",\n",
    "                          \"dist_normalized\",\n",
    "                          \"dist_normalized_var\",\n",
    "                          \"dist_normalized_std\",\n",
    "                          \"mean\",\n",
    "                          \"mean_std\"]}\n",
    "\n",
    "df_calc = pd.DataFrame()  # helper df for calculations\n",
    "\n",
    "for col_name in only_calc_col_names:\n",
    "    percentage_agreement_score = get_percentage_agreement_score(df_calc, df_r1[col_name], df_r2[col_name])\n",
    "\n",
    "    # calculate Cohen's Kappa (handle edge case when arrays are equal CK score should be 1)\n",
    "    ck_score = get_cohen_kappar_score(df_r1[col_name], df_r2[col_name])\n",
    "\n",
    "    # get krippendorff scores\n",
    "    krippendorff_score_ordinal = get_krippendorff_score(df_r1[col_name], df_r2[col_name], 'ordinal')\n",
    "\n",
    "    # distance between raters\n",
    "    dist = get_dist_score(df_calc, \"dist\", df_r1[col_name], df_r2[col_name])\n",
    "\n",
    "    # get normalized distance score\n",
    "    # 1. min-max normalize each rater's scores :\n",
    "    df_r1[f\"{col_name}_normalized\"] = calc_min_max_normalize(df_r1, col_name)\n",
    "    df_r2[f\"{col_name}_normalized\"] = calc_min_max_normalize(df_r2, col_name)\n",
    "    # 2. get normalized dist\n",
    "    norm_dist = get_dist_score(df_calc, \"norm_dist\", df_r1[f\"{col_name}_normalized\"], df_r2[f\"{col_name}_normalized\"])\n",
    "    # variance of normalized distance\n",
    "    dist_var = df_calc[\"norm_dist\"].var()\n",
    "    # std of normalized distance\n",
    "    dist_std = df_calc[\"norm_dist\"].std()\n",
    "\n",
    "    # get mean of both raters and then get\n",
    "    mean_score = get_raters_mean(df_r1[col_name], df_r2[col_name])\n",
    "    mean_std_score = get_raters_mean_std(df_r1[col_name], df_r2[col_name])\n",
    "\n",
    "    # add all to res dict\n",
    "    res_agreement[col_name] = [percentage_agreement_score,\n",
    "                               ck_score,\n",
    "                               krippendorff_score_ordinal,\n",
    "                               dist,\n",
    "                               norm_dist,\n",
    "                               dist_var,\n",
    "                               dist_std,\n",
    "                               mean_score,\n",
    "                               mean_std_score]\n",
    "\n",
    "res_df = pd.DataFrame(data=res_agreement)\n",
    "res_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "outputs": [
    {
     "data": {
      "text/plain": "test                        percentage_of_agreement cohen_kappa_score   \nFluency_target                                  1.0               1.0  \\\nFluency_t5detox                            0.921875          0.673469   \nFluency_t5formal                             0.9375          0.574043   \nFluency_t5large                            0.890625          0.621622   \nFluency_flanlarge                          0.921875           0.46932   \nCorpy_level_target                           0.9375          0.308108   \nCorpy_level_t5detox                        0.546875          0.346709   \nCorpy_level_t5formal                       0.578125          0.429138   \nCorpy_level_t5large                        0.515625          0.371555   \nCorpy_level_flanlarge                      0.359375          0.105048   \ncontent_intention_target                   0.953125         -0.021277   \ncontent_intention_t5detox                   0.53125          0.259259   \ncontent_intention_t5formal                 0.640625           0.40693   \ncontent_intention_t5large                      0.75          0.549296   \ncontent_intention_flanlarge                0.765625          0.609756   \nessential_details_target                    0.78125          0.197851   \nessential_details_t5detox                   0.59375          0.392257   \nessential_details_t5formal                 0.640625          0.439452   \nessential_details_t5large                  0.671875           0.44918   \nessential_details_flanlarge                 0.71875          0.558282   \n\ntest                        krippendorff_score_ordinal      dist   \nFluency_target                                     1.0       0.0  \\\nFluency_t5detox                               0.806287   0.09375   \nFluency_t5formal                              0.599294  0.078125   \nFluency_t5large                                0.73602     0.125   \nFluency_flanlarge                             0.568891  0.109375   \nCorpy_level_target                            0.300517   0.09375   \nCorpy_level_t5detox                           0.676956  0.703125   \nCorpy_level_t5formal                          0.769589     0.625   \nCorpy_level_t5large                           0.678615  0.734375   \nCorpy_level_flanlarge                         0.256045  1.203125   \ncontent_intention_target                        -0.016  0.046875   \ncontent_intention_t5detox                     0.365425   0.65625   \ncontent_intention_t5formal                    0.638775  0.421875   \ncontent_intention_t5large                     0.682651  0.328125   \ncontent_intention_flanlarge                   0.763135  0.296875   \nessential_details_target                      0.196987  0.265625   \nessential_details_t5detox                     0.631924  0.453125   \nessential_details_t5formal                    0.618098  0.421875   \nessential_details_t5large                     0.625769   0.40625   \nessential_details_flanlarge                   0.780767    0.3125   \n\ntest                        dist_normalized dist_normalized_var   \nFluency_target                          NaN                 NaN  \\\nFluency_t5detox                    0.046875            0.029514   \nFluency_t5formal                   0.039062            0.026228   \nFluency_t5large                      0.0625            0.035714   \nFluency_flanlarge                  0.054688            0.040613   \nCorpy_level_target                 0.052083            0.039572   \nCorpy_level_t5detox                0.175781            0.056904   \nCorpy_level_t5formal                0.15625            0.052579   \nCorpy_level_t5large                0.183594            0.060004   \nCorpy_level_flanlarge              0.300781            0.087658   \ncontent_intention_target           0.046875            0.045387   \ncontent_intention_t5detox          0.328125             0.15253   \ncontent_intention_t5formal         0.210938            0.093688   \ncontent_intention_t5large          0.164062            0.095672   \ncontent_intention_flanlarge        0.148438            0.084759   \nessential_details_target           0.132812            0.073351   \nessential_details_t5detox          0.226562            0.086744   \nessential_details_t5formal         0.210938            0.093688   \nessential_details_t5large          0.203125            0.100942   \nessential_details_flanlarge         0.15625            0.070437   \n\ntest                        dist_normalized_std      mean  mean_std  \nFluency_target                              NaN       2.0       0.0  \nFluency_t5detox                        0.171796  1.796875  0.524924  \nFluency_t5formal                        0.16195  1.867188  0.447588  \nFluency_t5large                        0.188982  1.765625   0.53429  \nFluency_flanlarge                      0.201526  1.882812  0.375248  \nCorpy_level_target                     0.198928    1.9375  0.243975  \nCorpy_level_t5detox                    0.238546  0.757812  1.394411  \nCorpy_level_t5formal                   0.229302  0.671875  1.348776  \nCorpy_level_t5large                    0.244958  0.414062  1.424153  \nCorpy_level_flanlarge                  0.296071  0.851562  1.060397  \ncontent_intention_target               0.213042  1.976562  0.106521  \ncontent_intention_t5detox              0.390551     1.125  0.740013  \ncontent_intention_t5formal             0.306085  1.273438  0.776284  \ncontent_intention_t5large              0.309309  1.304688  0.824319  \ncontent_intention_flanlarge            0.291135  0.960938  0.887768  \nessential_details_target               0.270833  1.820312   0.34923  \nessential_details_t5detox              0.294523  1.039062  0.783281  \nessential_details_t5formal             0.306085  1.195312  0.769545  \nessential_details_t5large              0.317714    1.3125  0.774084  \nessential_details_flanlarge            0.265399   0.90625  0.849253  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>test</th>\n      <th>percentage_of_agreement</th>\n      <th>cohen_kappa_score</th>\n      <th>krippendorff_score_ordinal</th>\n      <th>dist</th>\n      <th>dist_normalized</th>\n      <th>dist_normalized_var</th>\n      <th>dist_normalized_std</th>\n      <th>mean</th>\n      <th>mean_std</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Fluency_target</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Fluency_t5detox</th>\n      <td>0.921875</td>\n      <td>0.673469</td>\n      <td>0.806287</td>\n      <td>0.09375</td>\n      <td>0.046875</td>\n      <td>0.029514</td>\n      <td>0.171796</td>\n      <td>1.796875</td>\n      <td>0.524924</td>\n    </tr>\n    <tr>\n      <th>Fluency_t5formal</th>\n      <td>0.9375</td>\n      <td>0.574043</td>\n      <td>0.599294</td>\n      <td>0.078125</td>\n      <td>0.039062</td>\n      <td>0.026228</td>\n      <td>0.16195</td>\n      <td>1.867188</td>\n      <td>0.447588</td>\n    </tr>\n    <tr>\n      <th>Fluency_t5large</th>\n      <td>0.890625</td>\n      <td>0.621622</td>\n      <td>0.73602</td>\n      <td>0.125</td>\n      <td>0.0625</td>\n      <td>0.035714</td>\n      <td>0.188982</td>\n      <td>1.765625</td>\n      <td>0.53429</td>\n    </tr>\n    <tr>\n      <th>Fluency_flanlarge</th>\n      <td>0.921875</td>\n      <td>0.46932</td>\n      <td>0.568891</td>\n      <td>0.109375</td>\n      <td>0.054688</td>\n      <td>0.040613</td>\n      <td>0.201526</td>\n      <td>1.882812</td>\n      <td>0.375248</td>\n    </tr>\n    <tr>\n      <th>Corpy_level_target</th>\n      <td>0.9375</td>\n      <td>0.308108</td>\n      <td>0.300517</td>\n      <td>0.09375</td>\n      <td>0.052083</td>\n      <td>0.039572</td>\n      <td>0.198928</td>\n      <td>1.9375</td>\n      <td>0.243975</td>\n    </tr>\n    <tr>\n      <th>Corpy_level_t5detox</th>\n      <td>0.546875</td>\n      <td>0.346709</td>\n      <td>0.676956</td>\n      <td>0.703125</td>\n      <td>0.175781</td>\n      <td>0.056904</td>\n      <td>0.238546</td>\n      <td>0.757812</td>\n      <td>1.394411</td>\n    </tr>\n    <tr>\n      <th>Corpy_level_t5formal</th>\n      <td>0.578125</td>\n      <td>0.429138</td>\n      <td>0.769589</td>\n      <td>0.625</td>\n      <td>0.15625</td>\n      <td>0.052579</td>\n      <td>0.229302</td>\n      <td>0.671875</td>\n      <td>1.348776</td>\n    </tr>\n    <tr>\n      <th>Corpy_level_t5large</th>\n      <td>0.515625</td>\n      <td>0.371555</td>\n      <td>0.678615</td>\n      <td>0.734375</td>\n      <td>0.183594</td>\n      <td>0.060004</td>\n      <td>0.244958</td>\n      <td>0.414062</td>\n      <td>1.424153</td>\n    </tr>\n    <tr>\n      <th>Corpy_level_flanlarge</th>\n      <td>0.359375</td>\n      <td>0.105048</td>\n      <td>0.256045</td>\n      <td>1.203125</td>\n      <td>0.300781</td>\n      <td>0.087658</td>\n      <td>0.296071</td>\n      <td>0.851562</td>\n      <td>1.060397</td>\n    </tr>\n    <tr>\n      <th>content_intention_target</th>\n      <td>0.953125</td>\n      <td>-0.021277</td>\n      <td>-0.016</td>\n      <td>0.046875</td>\n      <td>0.046875</td>\n      <td>0.045387</td>\n      <td>0.213042</td>\n      <td>1.976562</td>\n      <td>0.106521</td>\n    </tr>\n    <tr>\n      <th>content_intention_t5detox</th>\n      <td>0.53125</td>\n      <td>0.259259</td>\n      <td>0.365425</td>\n      <td>0.65625</td>\n      <td>0.328125</td>\n      <td>0.15253</td>\n      <td>0.390551</td>\n      <td>1.125</td>\n      <td>0.740013</td>\n    </tr>\n    <tr>\n      <th>content_intention_t5formal</th>\n      <td>0.640625</td>\n      <td>0.40693</td>\n      <td>0.638775</td>\n      <td>0.421875</td>\n      <td>0.210938</td>\n      <td>0.093688</td>\n      <td>0.306085</td>\n      <td>1.273438</td>\n      <td>0.776284</td>\n    </tr>\n    <tr>\n      <th>content_intention_t5large</th>\n      <td>0.75</td>\n      <td>0.549296</td>\n      <td>0.682651</td>\n      <td>0.328125</td>\n      <td>0.164062</td>\n      <td>0.095672</td>\n      <td>0.309309</td>\n      <td>1.304688</td>\n      <td>0.824319</td>\n    </tr>\n    <tr>\n      <th>content_intention_flanlarge</th>\n      <td>0.765625</td>\n      <td>0.609756</td>\n      <td>0.763135</td>\n      <td>0.296875</td>\n      <td>0.148438</td>\n      <td>0.084759</td>\n      <td>0.291135</td>\n      <td>0.960938</td>\n      <td>0.887768</td>\n    </tr>\n    <tr>\n      <th>essential_details_target</th>\n      <td>0.78125</td>\n      <td>0.197851</td>\n      <td>0.196987</td>\n      <td>0.265625</td>\n      <td>0.132812</td>\n      <td>0.073351</td>\n      <td>0.270833</td>\n      <td>1.820312</td>\n      <td>0.34923</td>\n    </tr>\n    <tr>\n      <th>essential_details_t5detox</th>\n      <td>0.59375</td>\n      <td>0.392257</td>\n      <td>0.631924</td>\n      <td>0.453125</td>\n      <td>0.226562</td>\n      <td>0.086744</td>\n      <td>0.294523</td>\n      <td>1.039062</td>\n      <td>0.783281</td>\n    </tr>\n    <tr>\n      <th>essential_details_t5formal</th>\n      <td>0.640625</td>\n      <td>0.439452</td>\n      <td>0.618098</td>\n      <td>0.421875</td>\n      <td>0.210938</td>\n      <td>0.093688</td>\n      <td>0.306085</td>\n      <td>1.195312</td>\n      <td>0.769545</td>\n    </tr>\n    <tr>\n      <th>essential_details_t5large</th>\n      <td>0.671875</td>\n      <td>0.44918</td>\n      <td>0.625769</td>\n      <td>0.40625</td>\n      <td>0.203125</td>\n      <td>0.100942</td>\n      <td>0.317714</td>\n      <td>1.3125</td>\n      <td>0.774084</td>\n    </tr>\n    <tr>\n      <th>essential_details_flanlarge</th>\n      <td>0.71875</td>\n      <td>0.558282</td>\n      <td>0.780767</td>\n      <td>0.3125</td>\n      <td>0.15625</td>\n      <td>0.070437</td>\n      <td>0.265399</td>\n      <td>0.90625</td>\n      <td>0.849253</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_df = res_df.T\n",
    "res_df.columns = res_df.iloc[0]\n",
    "res_df = res_df.drop(res_df.index[0])\n",
    "res_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "outputs": [],
   "source": [
    "res_df.to_csv(r\"Evaluation_Data_And_Annotation/annotators_agreement_and_mean_scores.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
